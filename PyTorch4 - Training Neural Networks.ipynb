{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training pass consists of four different steps above\n",
    "- Forward pass through the network\n",
    "- Calculate the loss using network output\n",
    "- Backwards pass through the network with loss.backwards -> calculates the gradients\n",
    "- Make a step with our optimizer that updates the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# TO train a network we need to measure our loss or error \n",
    "# We use loss function to see how poorly the network is classicfiying\n",
    "# The mean squared loss is often used in regression and binary classification problems\n",
    "# The gradient is the slope of the loss function and points in the direction of fastest change\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "                               ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3344, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128,64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64,10)\n",
    "                     )\n",
    "\n",
    "#Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2956, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Another method to build above network\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1)\n",
    "                     )\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits =  model(images)\n",
    "\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to calculate a loss, how do we use it to perform backpropagation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch provide Autograd to calculate gradients of the tensors\n",
    "# Autograd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss and Autograd together\n",
    "Autograd calculates the gradients of our tensors automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3063, grad_fn=<NllLossBackward>)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784,128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128,64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64,10),\n",
    "                      nn.LogSoftmax(dim=1)\n",
    "                     )\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images)\n",
    "loss = criterion(logps, labels)\n",
    "print(loss)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[ 0.0033,  0.0033,  0.0033,  ...,  0.0033,  0.0033,  0.0033],\n",
      "        [ 0.0029,  0.0029,  0.0029,  ...,  0.0029,  0.0029,  0.0029],\n",
      "        [-0.0016, -0.0016, -0.0016,  ..., -0.0016, -0.0016, -0.0016],\n",
      "        ...,\n",
      "        [ 0.0045,  0.0045,  0.0045,  ...,  0.0045,  0.0045,  0.0045],\n",
      "        [ 0.0012,  0.0012,  0.0012,  ...,  0.0012,  0.0012,  0.0012],\n",
      "        [ 0.0010,  0.0010,  0.0010,  ...,  0.0010,  0.0010,  0.0010]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "loss.backward() # Calculates the gradients auto\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer function\n",
    "There's one last piece we need to start training, an optimizer that we'll use to update the weights witht he gradients. We get these from PyTorch's optim package.\n",
    "Example: optim.SGD -> stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[-0.0176, -0.0003, -0.0280,  ..., -0.0319, -0.0272,  0.0054],\n",
      "        [-0.0179, -0.0206,  0.0050,  ...,  0.0230,  0.0008, -0.0298],\n",
      "        [ 0.0280,  0.0277, -0.0294,  ...,  0.0267, -0.0147, -0.0078],\n",
      "        ...,\n",
      "        [-0.0229,  0.0182, -0.0318,  ..., -0.0140, -0.0195, -0.0288],\n",
      "        [-0.0019, -0.0245, -0.0057,  ..., -0.0297,  0.0281,  0.0216],\n",
      "        [-0.0063,  0.0168, -0.0187,  ..., -0.0227, -0.0322,  0.0059]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[ 0.0051,  0.0051,  0.0051,  ...,  0.0051,  0.0051,  0.0051],\n",
      "        [ 0.0027,  0.0027,  0.0027,  ...,  0.0027,  0.0027,  0.0027],\n",
      "        [-0.0025, -0.0025, -0.0025,  ..., -0.0025, -0.0025, -0.0025],\n",
      "        ...,\n",
      "        [ 0.0018,  0.0018,  0.0018,  ...,  0.0018,  0.0018,  0.0018],\n",
      "        [ 0.0009,  0.0009,  0.0009,  ...,  0.0009,  0.0009,  0.0009],\n",
      "        [ 0.0013,  0.0013,  0.0013,  ...,  0.0013,  0.0013,  0.0013]])\n",
      "Updated weights -  Parameter containing:\n",
      "tensor([[-0.0176, -0.0003, -0.0280,  ..., -0.0320, -0.0273,  0.0053],\n",
      "        [-0.0179, -0.0206,  0.0049,  ...,  0.0230,  0.0008, -0.0299],\n",
      "        [ 0.0281,  0.0277, -0.0294,  ...,  0.0267, -0.0147, -0.0077],\n",
      "        ...,\n",
      "        [-0.0230,  0.0182, -0.0318,  ..., -0.0140, -0.0195, -0.0289],\n",
      "        [-0.0019, -0.0245, -0.0058,  ..., -0.0297,  0.0281,  0.0216],\n",
      "        [-0.0063,  0.0168, -0.0187,  ..., -0.0227, -0.0322,  0.0059]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizer require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print('Initial weights - ', model[0].weight)\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model[0].weight.grad)\n",
    "\n",
    "# Take an update step and few the new weights\n",
    "optimizer.step()\n",
    "print('Updated weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training pass consists of four different steps above\n",
    "- Forward pass through the network\n",
    "- Calculate the loss using network output\n",
    "- Backwards pass through the network with loss.backwards -> calculates the gradients\n",
    "- Make a step with our optimizer that updates the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.8547626563480921\n",
      "Training loss: 0.8068775440584114\n",
      "Training loss: 0.5065598223827033\n",
      "Training loss: 0.4183633315728417\n",
      "Training loss: 0.376767489844675\n",
      "Training loss: 0.3511403638527973\n",
      "Training loss: 0.3332080350501705\n",
      "Training loss: 0.3196052328181038\n",
      "Training loss: 0.30826130015318837\n",
      "Training loss: 0.2981893922696744\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784,128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128,64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64,10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # TODO: Training pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the network trained, we can check out it's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1.1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "images, lables = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784) # getting one image to classify and see if correct or not\n",
    "\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "#Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "ps = ps.data.numpy().squeeze()\n",
    "\n",
    "# Generating images\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(6,7), ncols=2)\n",
    "\n",
    "ax1.imshow(images[0].numpy().squeeze())\n",
    "ax1.axis('off')\n",
    "\n",
    "\n",
    "ax2.barh(np.arange(10), ps)\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(10))\n",
    "ax2.set_yticklabels(np.arange(10).astype(int), size='large');\n",
    "ax2.set_title('Digit Probability')\n",
    "ax2.set_xlim(0, 1.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
